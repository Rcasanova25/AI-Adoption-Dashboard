#!/usr/bin/env python3
"""
Quick Integration Test - Minimal dependency check
Tests core integration without requiring external packages
"""

import sys
from pathlib import Path

def test_critical_imports():
    """Test that our critical integration points work"""
    print("ğŸ” Testing critical integration points...")
    
    project_root = Path(__file__).parent
    sys.path.insert(0, str(project_root))
    
    try:
        # Test 1: Can we import our research integration?
        sys.path.insert(0, str(project_root / "data"))
        
        print("ğŸ“‹ Testing research integration import...")
        with open(project_root / "data" / "research_integration.py", 'r') as f:
            content = f.read()
            
        # Check for key class definition
        if "class ResearchDataIntegrator:" in content:
            print("âœ… ResearchDataIntegrator class found")
        else:
            print("âŒ ResearchDataIntegrator class missing")
            return False
            
        # Test 2: Check data loaders integration
        print("ğŸ“‹ Testing data loaders integration...")
        with open(project_root / "data" / "loaders.py", 'r') as f:
            loaders_content = f.read()
            
        if "from .research_integration import" in loaders_content:
            print("âœ… Research integration imported in loaders")
        else:
            print("âŒ Research integration not imported in loaders")
            return False
            
        # Test 3: Check main.py integration
        print("ğŸ“‹ Testing main.py integration...")
        with open(project_root / "main.py", 'r') as f:
            main_content = f.read()
            
        if "load_authentic_dashboard_data" in main_content:
            print("âœ… Authentic data loading integrated in main.py")
        else:
            print("âŒ Authentic data loading not integrated in main.py")
            return False
            
        if "authentic research data from Stanford" in main_content:
            print("âœ… Source attribution UI integrated")
        else:
            print("âŒ Source attribution UI not integrated")
            return False
            
        # Test 4: Check data source configuration
        print("ğŸ“‹ Testing data source configuration...")
        if "Stanford AI Index Report 2025" in content:
            print("âœ… Stanford AI Index configured")
        else:
            print("âŒ Stanford AI Index not configured")
            return False
            
        if "McKinsey Global Survey" in content:
            print("âœ… McKinsey Global Survey configured")  
        else:
            print("âŒ McKinsey Global Survey not configured")
            return False
            
        if "Goldman Sachs Research" in content:
            print("âœ… Goldman Sachs Research configured")
        else:
            print("âŒ Goldman Sachs Research not configured")
            return False
            
        if "Federal Reserve Bank" in content:
            print("âœ… Federal Reserve Research configured")
        else:
            print("âŒ Federal Reserve Research not configured")
            return False
            
        return True
        
    except Exception as e:
        print(f"âŒ Critical import test failed: {e}")
        return False

def test_data_authenticity_flow():
    """Test the data authenticity verification flow"""
    print("\nğŸ” Testing data authenticity flow...")
    
    try:
        project_root = Path(__file__).parent
        
        # Check research integration file
        research_file = project_root / "data" / "research_integration.py"
        with open(research_file, 'r') as f:
            content = f.read()
            
        # Test authenticity methods
        authenticity_methods = [
            "get_authentic_historical_data",
            "get_authentic_sector_data_2025", 
            "get_authentic_investment_data",
            "get_authentic_financial_impact_data",
            "get_authentic_productivity_data",
            "get_authentic_gdp_impact_data"
        ]
        
        missing_methods = []
        
        for method in authenticity_methods:
            if method in content:
                print(f"âœ… {method}")
            else:
                print(f"âŒ {method} - MISSING")
                missing_methods.append(method)
                
        if missing_methods:
            print(f"âŒ Missing {len(missing_methods)} authenticity methods")
            return False
            
        # Test credibility reporting
        if "get_data_lineage_report" in content:
            print("âœ… Data lineage reporting")
        else:
            print("âŒ Data lineage reporting - MISSING")
            return False
            
        # Test source attribution
        if "data_source" in content and "credibility_score" in content:
            print("âœ… Source attribution system")
        else:
            print("âŒ Source attribution system - MISSING")
            return False
            
        return True
        
    except Exception as e:
        print(f"âŒ Data authenticity flow test failed: {e}")
        return False

def test_fallback_system():
    """Test that fallback system is properly configured"""
    print("\nğŸ” Testing fallback system...")
    
    try:
        project_root = Path(__file__).parent
        
        # Check main.py for fallback handling
        with open(project_root / "main.py", 'r') as f:
            main_content = f.read()
            
        # Test fallback data function
        if "load_fallback_data" in main_content:
            print("âœ… Fallback data function exists")
        else:
            print("âŒ Fallback data function missing")
            return False
            
        # Test fallback messaging
        if "synthetic fallback data" in main_content:
            print("âœ… Fallback messaging configured")
        else:
            print("âŒ Fallback messaging missing")
            return False
            
        # Check loaders.py for fallback handling
        with open(project_root / "data" / "loaders.py", 'r') as f:
            loaders_content = f.read()
            
        if "fallback_data" in loaders_content:
            print("âœ… Loader fallback system configured")
        else:
            print("âŒ Loader fallback system missing")
            return False
            
        return True
        
    except Exception as e:
        print(f"âŒ Fallback system test failed: {e}")
        return False

def run_quick_test():
    """Run quick integration test"""
    print("âš¡ Quick Integration Test")
    print("=" * 40)
    
    tests = [
        ("Critical Imports", test_critical_imports),
        ("Data Authenticity Flow", test_data_authenticity_flow),
        ("Fallback System", test_fallback_system)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        result = test_func()
        results.append((test_name, result))
    
    # Summary
    print("\n" + "=" * 40)
    print("âš¡ QUICK TEST SUMMARY")
    print("=" * 40)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… PASSED" if result else "âŒ FAILED"
        print(f"{status} {test_name}")
    
    print(f"\nğŸ“Š Overall: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nğŸ‰ INTEGRATION READY!")
        print("ğŸš€ Your authentic data integration is properly configured")
        print("ğŸš€ Ready to run: streamlit run main.py")
        print("\nğŸ“‹ Expected results:")
        print("   âœ… Green banner with Stanford/McKinsey/Goldman Sachs sources")
        print("   âœ… Data Authenticity Verification panel")
        print("   âœ… A+ credibility score")
        print("   âœ… Source attribution in all datasets")
    else:
        print("\nâš ï¸ Integration issues detected")
        print("ğŸ”§ Review failed tests and fix before dashboard testing")
    
    return passed == total

if __name__ == "__main__":
    success = run_quick_test()
    sys.exit(0 if success else 1)