# -*- coding: utf-8 -*-
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from datetime import datetime
from plotly.subplots import make_subplots

# Page config
st.set_page_config(
    page_title="AI Adoption Dashboard | 2018-2025 Analysis",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://github.com/Rcasanova25/AI-Adoption-Dashboard/wiki',
        'Report a bug': "https://github.com/Rcasanova25/AI-Adoption-Dashboard/issues",
        'About': "# AI Adoption Dashboard\nVersion 2.3.0\n\nTrack AI adoption trends across industries and geographies with token economics insights.\n\nCreated by Robert Casanova"
    }
)

# Data loading function - updated with AI Index 2025 data and token economics
@st.cache_data
def load_data():
    # Historical trends data - UPDATED with AI Index 2025 findings
    historical_data = pd.DataFrame({
        'year': [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025],
        'ai_use': [20, 47, 58, 56, 55, 50, 55, 78, 78],  # Updated: 78% in 2024
        'genai_use': [0, 0, 0, 0, 0, 33, 33, 71, 71]  # Updated: 71% in 2024
    })
    
    # Token pricing evolution data
    token_pricing = pd.DataFrame({
        'date': ['Nov 2022', 'Jan 2023', 'Jul 2023', 'Jan 2024', 'Oct 2024', 'Oct 2024 (Gemini)'],
        'cost_per_million': [20.00, 10.00, 2.00, 0.50, 0.14, 0.07],
        'cost_reduction': [1, 2, 10, 40, 143, 286],
        'billion_token_cost': [20000, 10000, 2000, 500, 140, 70]
    })
    
    # Token usage patterns
    token_usage = pd.DataFrame({
        'use_case': ['Chat (simple)', 'Email generation', 'Code review', 'Document analysis', 
                    'Research summary', 'Contract review', 'Book summary', 'Video analysis'],
        'avg_tokens': [500, 1000, 2000, 10000, 5000, 15000, 50000, 100000],
        'typical_time_min': [0.5, 1, 2, 5, 5, 10, 15, 20],
        'value_category': ['Low', 'Low', 'Medium', 'Medium', 'High', 'High', 'Very High', 'Very High']
    })
    
    # 2018 Sector data
    sector_2018 = pd.DataFrame({
        'sector': ['Manufacturing', 'Information', 'Healthcare', 'Professional Services', 
                  'Finance & Insurance', 'Retail Trade', 'Construction'],
        'firm_weighted': [12, 12, 8, 7, 6, 4, 4],
        'employment_weighted': [18, 22, 15, 14, 12, 8, 6]
    })
    
    # 2025 Sector data with token usage intensity
    sector_2025 = pd.DataFrame({
        'sector': ['Technology', 'Financial Services', 'Healthcare', 'Manufacturing', 
                  'Retail & E-commerce', 'Education', 'Energy & Utilities', 'Government'],
        'adoption_rate': [92, 85, 78, 75, 72, 65, 58, 52],
        'genai_adoption': [88, 78, 65, 58, 70, 62, 45, 38],
        'avg_roi': [4.2, 3.8, 3.2, 3.5, 3.0, 2.5, 2.8, 2.2],
        'avg_monthly_tokens_millions': [850, 720, 450, 380, 620, 280, 210, 150],
        'token_cost_savings_pct': [95, 93, 91, 88, 92, 87, 85, 82]
    })
    
    # Firm size data with token usage
    firm_size = pd.DataFrame({
        'size': ['1-4', '5-9', '10-19', '20-49', '50-99', '100-249', '250-499', 
                '500-999', '1000-2499', '2500-4999', '5000+'],
        'adoption': [3.2, 3.8, 4.5, 5.2, 7.8, 12.5, 18.2, 25.6, 35.4, 42.8, 58.5],
        'avg_tokens_per_employee_daily': [10, 25, 50, 100, 200, 350, 500, 750, 1000, 1500, 2000]
    })
    
    # AI Maturity data with token requirements
    ai_maturity = pd.DataFrame({
        'technology': ['Generative AI', 'AI Agents', 'Foundation Models', 'ModelOps', 
                      'AI Engineering', 'Cloud AI Services', 'Knowledge Graphs', 'Composite AI'],
        'adoption_rate': [71, 15, 45, 25, 30, 78, 35, 12],
        'maturity': ['Peak of Expectations', 'Peak of Expectations', 'Trough of Disillusionment',
                    'Trough of Disillusionment', 'Peak of Expectations', 'Slope of Enlightenment',
                    'Slope of Enlightenment', 'Peak of Expectations'],
        'risk_score': [85, 90, 60, 55, 80, 25, 40, 95],
        'time_to_value': [3, 3, 3, 3, 3, 1, 3, 7],
        'avg_tokens_per_query': [2000, 5000, 10000, 500, 1000, 1500, 3000, 15000]
    })
    
    # Geographic data - enhanced with population and GDP
    geographic = pd.DataFrame({
        'city': ['San Francisco Bay Area', 'Nashville', 'San Antonio', 'Las Vegas', 
                'New Orleans', 'San Diego', 'Seattle', 'Boston', 'Los Angeles',
                'Phoenix', 'Denver', 'Austin', 'Portland', 'Miami', 'Atlanta',
                'Chicago', 'New York', 'Philadelphia', 'Dallas', 'Houston'],
        'state': ['California', 'Tennessee', 'Texas', 'Nevada', 
                 'Louisiana', 'California', 'Washington', 'Massachusetts', 'California',
                 'Arizona', 'Colorado', 'Texas', 'Oregon', 'Florida', 'Georgia',
                 'Illinois', 'New York', 'Pennsylvania', 'Texas', 'Texas'],
        'lat': [37.7749, 36.1627, 29.4241, 36.1699, 
               29.9511, 32.7157, 47.6062, 42.3601, 34.0522,
               33.4484, 39.7392, 30.2672, 45.5152, 25.7617, 33.7490,
               41.8781, 40.7128, 39.9526, 32.7767, 29.7604],
        'lon': [-122.4194, -86.7816, -98.4936, -115.1398, 
               -90.0715, -117.1611, -122.3321, -71.0589, -118.2437,
               -112.0740, -104.9903, -97.7431, -122.6784, -80.1918, -84.3880,
               -87.6298, -74.0060, -75.1652, -96.7970, -95.3698],
        'rate': [9.5, 8.3, 8.3, 7.7, 
                7.4, 7.4, 6.8, 6.7, 7.2,
                6.5, 6.3, 7.8, 6.2, 6.9, 7.1,
                7.0, 8.0, 6.6, 7.5, 7.3],
        'state_code': ['CA', 'TN', 'TX', 'NV', 
                      'LA', 'CA', 'WA', 'MA', 'CA',
                      'AZ', 'CO', 'TX', 'OR', 'FL', 'GA',
                      'IL', 'NY', 'PA', 'TX', 'TX'],
        'population_millions': [7.7, 0.7, 1.5, 0.6, 
                               0.4, 1.4, 0.8, 0.7, 4.0,
                               1.7, 0.7, 1.0, 0.7, 0.5, 0.5,
                               2.7, 8.3, 1.6, 1.3, 2.3],
        'gdp_billions': [535, 48, 98, 68, 
                        25, 253, 392, 463, 860,
                        162, 201, 148, 121, 345, 396,
                        610, 1487, 388, 368, 356]
    })
    
    # State-level aggregation
    state_data = geographic.groupby(['state', 'state_code']).agg({
        'rate': 'mean'
    }).reset_index()
    
    # Add more states
    additional_states = pd.DataFrame({
        'state': ['Michigan', 'Ohio', 'North Carolina', 'Virginia', 'Maryland',
                 'Connecticut', 'New Jersey', 'Indiana', 'Missouri', 'Wisconsin'],
        'state_code': ['MI', 'OH', 'NC', 'VA', 'MD', 'CT', 'NJ', 'IN', 'MO', 'WI'],
        'rate': [5.5, 5.8, 6.0, 6.2, 6.4, 6.8, 6.9, 5.2, 5.4, 5.3]
    })
    state_data = pd.concat([state_data, additional_states], ignore_index=True)
    
    # Technology stack with token efficiency
    tech_stack = pd.DataFrame({
        'technology': ['AI Only', 'AI + Cloud', 'AI + Digitization', 'AI + Cloud + Digitization'],
        'percentage': [15, 45, 38, 62],
        'token_efficiency_multiplier': [1.0, 2.5, 2.2, 3.8]
    })
    
    # Productivity data with skill levels - ENHANCED
    productivity_data = pd.DataFrame({
        'year': [1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020, 2025],
        'productivity_growth': [0.8, 1.2, 1.5, 2.2, 2.5, 1.8, 1.0, 0.5, 0.3, 0.4],
        'young_workers_share': [42, 45, 43, 38, 35, 33, 32, 34, 36, 38]
    })
    
    # AI productivity by skill level with token usage
    productivity_by_skill = pd.DataFrame({
        'skill_level': ['Low-skilled', 'Medium-skilled', 'High-skilled'],
        'productivity_gain': [14, 9, 5],
        'skill_gap_reduction': [28, 18, 8],
        'avg_daily_tokens': [5000, 15000, 25000]
    })
    
    # AI productivity estimates
    ai_productivity_estimates = pd.DataFrame({
        'source': ['Acemoglu (2024)', 'Brynjolfsson et al. (2023)', 'McKinsey (potential)', 'Goldman Sachs (potential)', 'Richmond Fed'],
        'annual_impact': [0.07, 1.5, 2.0, 2.5, 0.1]
    })
    
    # OECD 2025 Report data
    oecd_g7_adoption = pd.DataFrame({
        'country': ['United States', 'Canada', 'United Kingdom', 'Germany', 'France', 'Italy', 'Japan'],
        'adoption_rate': [45, 38, 42, 40, 35, 32, 48],
        'manufacturing': [52, 45, 48, 55, 42, 40, 58],
        'ict_sector': [68, 62, 65, 63, 58, 55, 70]
    })
    
    # OECD AI Applications with token intensity
    oecd_applications = pd.DataFrame({
        'application': ['Content Generation', 'Code Generation', 'Customer Service Chatbots',
                       'Predictive Maintenance', 'Process Automation', 'Customer Analytics', 
                       'Quality Control', 'Supply Chain Optimization', 'Fraud Detection',
                       'Product Recommendation', 'Voice Recognition', 'Computer Vision',
                       'Natural Language Processing', 'Robotics Integration', 'Personalized Learning'],
        'usage_rate': [65, 58, 52, 45, 42, 38, 35, 32, 30, 28, 25, 23, 22, 18, 15],
        'category': ['GenAI', 'GenAI', 'GenAI', 'Traditional AI', 'Traditional AI', 
                    'Traditional AI', 'Traditional AI', 'Traditional AI', 'Traditional AI',
                    'Traditional AI', 'Traditional AI', 'Traditional AI', 'Traditional AI', 
                    'Traditional AI', 'GenAI'],
        'avg_tokens_per_use': [5000, 3000, 2000, 500, 200, 1000, 300, 1500, 800, 100, 1500, 2000, 3000, 50, 10000]
    })
    
    # Barriers to AI Adoption
    barriers_data = pd.DataFrame({
        'barrier': ['Lack of skilled personnel', 'Data availability/quality', 'Integration with legacy systems',
                   'Regulatory uncertainty', 'High implementation costs', 'Security concerns',
                   'Unclear ROI', 'Organizational resistance'],
        'percentage': [68, 62, 58, 55, 52, 48, 45, 40]
    })
    
    # Support effectiveness
    support_effectiveness = pd.DataFrame({
        'support_type': ['Government education investment', 'University partnerships', 
                        'Public-private collaboration', 'Regulatory clarity',
                        'Tax incentives', 'Innovation grants', 'Technology centers'],
        'effectiveness_score': [82, 78, 75, 73, 68, 65, 62]
    })
    
    # AI Investment data with token context
    ai_investment_data = pd.DataFrame({
        'year': [2014, 2020, 2021, 2022, 2023, 2024],
        'total_investment': [19.4, 72.5, 112.3, 148.5, 174.6, 252.3],
        'genai_investment': [0, 0, 0, 3.95, 28.5, 33.9],
        'us_investment': [8.5, 31.2, 48.7, 64.3, 75.6, 109.1],
        'china_investment': [1.2, 5.8, 7.1, 7.9, 8.4, 9.3],
        'uk_investment': [0.3, 1.8, 2.5, 3.2, 3.8, 4.5],
        'cost_per_million_tokens': [100, 50, 30, 20, 2, 0.14]
    })
    
    # Regional AI adoption growth from AI Index 2025
    regional_growth = pd.DataFrame({
        'region': ['Greater China', 'Europe', 'North America', 'Asia-Pacific', 'Latin America'],
        'growth_2024': [27, 23, 15, 18, 12],
        'adoption_rate': [68, 65, 82, 58, 45],
        'investment_growth': [32, 28, 44, 25, 18]
    })
    
    # AI cost reduction data with token context
    ai_cost_reduction = pd.DataFrame({
        'model': ['GPT-3.5 (Nov 2022)', 'GPT-3.5 (Oct 2024)', 'Gemini-1.5-Flash-8B'],
        'cost_per_million_tokens': [20.00, 0.14, 0.07],
        'year': [2022, 2024, 2024],
        'tokens_per_dollar': [50000, 7142857, 14285714]
    })
    
    # Financial impact by function with token ROI
    financial_impact = pd.DataFrame({
        'function': ['Marketing & Sales', 'Service Operations', 'Supply Chain', 'Software Engineering', 
                    'Product Development', 'IT', 'HR', 'Finance'],
        'companies_reporting_cost_savings': [38, 49, 43, 41, 35, 37, 28, 32],
        'companies_reporting_revenue_gains': [71, 57, 63, 45, 52, 40, 35, 38],
        'avg_cost_reduction': [7, 8, 9, 10, 6, 7, 5, 6],
        'avg_revenue_increase': [4, 3, 4, 3, 4, 3, 2, 3],
        'monthly_tokens_millions': [120, 85, 65, 150, 95, 110, 45, 55]
    })
    
    # Generational AI perception data from AI Index 2025
    ai_perception = pd.DataFrame({
        'generation': ['Gen Z', 'Millennials', 'Gen X', 'Baby Boomers'],
        'expect_job_change': [67, 65, 58, 49],
        'expect_job_replacement': [42, 40, 34, 28]
    })
    
    # Training emissions data with token count
    training_emissions = pd.DataFrame({
        'model': ['AlexNet (2012)', 'GPT-3 (2020)', 'GPT-4 (2023)', 'Llama 3.1 405B (2024)'],
        'carbon_tons': [0.01, 588, 5184, 8930],
        'training_tokens_billions': [0.001, 300, 13000, 15000]
    })
    
    # Skill gap data
    skill_gap_data = pd.DataFrame({
        'skill': ['AI/ML Engineering', 'Data Science', 'AI Ethics', 'Prompt Engineering',
                 'AI Product Management', 'MLOps', 'AI Security', 'Change Management'],
        'gap_severity': [85, 78, 72, 68, 65, 62, 58, 55],
        'training_initiatives': [45, 52, 28, 38, 32, 35, 22, 48]
    })
    
    # AI governance data
    ai_governance = pd.DataFrame({
        'aspect': ['Ethics Guidelines', 'Data Privacy', 'Bias Detection', 'Transparency',
                  'Accountability Framework', 'Risk Assessment', 'Regulatory Compliance'],
        'adoption_rate': [62, 78, 45, 52, 48, 55, 72],
        'maturity_score': [3.2, 3.8, 2.5, 2.8, 2.6, 3.0, 3.5]
    })
    
    # 2025 GenAI by function (for backward compatibility)
    genai_2025 = pd.DataFrame({
        'function': ['Marketing & Sales', 'Product Development', 'Service Operations', 
                    'Software Engineering', 'IT', 'Knowledge Management', 'HR', 'Supply Chain'],
        'adoption': [42, 28, 23, 22, 23, 21, 13, 7]
    })
    
    # Token performance metrics
    token_performance = pd.DataFrame({
        'metric': ['Time to First Token', 'Inter-token Latency', 'Tokens per Second', 'Context Window Size'],
        'unit': ['milliseconds', 'milliseconds', 'tokens/sec', 'tokens'],
        'good_performance': [200, 20, 100, 100000],
        'average_performance': [500, 50, 50, 32000],
        'poor_performance': [1000, 100, 20, 8000]
    })
    
    return (historical_data, sector_2018, sector_2025, firm_size, ai_maturity, 
            geographic, tech_stack, productivity_data, productivity_by_skill,
            ai_productivity_estimates, oecd_g7_adoption, oecd_applications, 
            barriers_data, support_effectiveness, state_data, ai_investment_data, 
            regional_growth, ai_cost_reduction, financial_impact, ai_perception, 
            training_emissions, skill_gap_data, ai_governance, genai_2025,
            token_pricing, token_usage, token_performance)

# Initialize session state
if 'first_visit' not in st.session_state:
    st.session_state.first_visit = True
if 'selected_persona' not in st.session_state:
    st.session_state.selected_persona = "General"
if 'show_changelog' not in st.session_state:
    st.session_state.show_changelog = False
if 'year_filter' not in st.session_state:
    st.session_state.year_filter = None
if 'compare_years' not in st.session_state:
    st.session_state.compare_years = False
if 'show_token_info' not in st.session_state:
    st.session_state.show_token_info = False

# Helper function for source info
def show_source_info(source_key):
    sources = {
        'ai_index': {
            'title': 'AI Index Report 2025',
            'org': 'Stanford HAI',
            'url': 'https://aiindex.stanford.edu',
            'methodology': 'Comprehensive analysis of AI metrics globally'
        },
        'mckinsey': {
            'title': 'McKinsey Global Survey on AI',
            'org': 'McKinsey & Company',
            'url': 'https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai',
            'methodology': '1,491 participants across 101 nations, July 2024'
        },
        'oecd': {
            'title': 'OECD/BCG/INSEAD Report 2025',
            'org': 'OECD AI Policy Observatory',
            'url': 'https://oecd.ai',
            'methodology': '840 enterprises across G7 countries + Brazil'
        },
        'census': {
            'title': 'US Census Bureau AI Use Supplement',
            'org': 'US Census Bureau',
            'url': 'https://www.census.gov',
            'methodology': '850,000 U.S. firms surveyed'
        },
        'nvidia': {
            'title': 'NVIDIA AI Tokens Explained',
            'org': 'NVIDIA Corporation',
            'url': 'https://blogs.nvidia.com/blog/ai-tokens-explained/',
            'methodology': 'Technical analysis of token economics and AI factories'
        }
    }
    
    if source_key in sources:
    source = sources[source_key]
    return f"""
**Source:** {source['title']}
**Organization:** {source['org']}
**Methodology:** {source['methodology']}
[View Report]({source['url']})
"""
return ""

# Token information helper
def show_token_explainer():
    return st.info("""
    **ü™ô What are Tokens?** Tokens are tiny units of data that AI models process. Think of them as the "words" AI understands:
    ‚Ä¢ Short words = 1 token (e.g., "cat", "run")
    ‚Ä¢ Longer words = multiple tokens (e.g., "understanding" = "under" + "stand" + "ing")
    ‚Ä¢ 1,000 tokens ‚âà 750 words ‚âà 1.5 pages of text
    ‚Ä¢ Images, audio, and video are also converted to tokens for processing
    
    **Why do tokens matter?** AI services charge by tokens processed, making the 280x cost reduction crucial for adoption.
    """)

# Onboarding modal for first-time users
if st.session_state.first_visit:
    with st.container():
        st.info("""
        ### üëã Welcome to the AI Adoption Dashboard!
        
        This dashboard provides comprehensive insights into AI adoption trends from 2018-2025, 
        including the latest findings from the AI Index Report 2025 and token economics analysis.
        
        **Quick Start:**
        - Use the sidebar to select different analysis views
        - Click on charts to see detailed information
        - Look for ü™ô icons to learn about token economics
        - Export any visualization using the download buttons
        
        **For best experience, select your role:**
        """)
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            if st.button("üìä Business Leader"):
                st.session_state.selected_persona = "Business Leader"
                st.session_state.first_visit = False
                st.rerun()
        with col2:
            if st.button("üèõÔ∏è Policymaker"):
                st.session_state.selected_persona = "Policymaker"
                st.session_state.first_visit = False
                st.rerun()
        with col3:
            if st.button("üî¨ Researcher"):
                st.session_state.selected_persona = "Researcher"
                st.session_state.first_visit = False
                st.rerun()
        with col4:
            if st.button("üë§ General User"):
                st.session_state.selected_persona = "General"
                st.session_state.first_visit = False
                st.rerun()
        
        if st.button("Got it! Let's explore", type="primary"):
            st.session_state.first_visit = False
            st.rerun()
    st.stop()

# Load all data
try:
    loaded_data = load_data()
    
    # Unpack the data
    (historical_data, sector_2018, sector_2025, firm_size, ai_maturity, 
     geographic, tech_stack, productivity_data, productivity_by_skill,
     ai_productivity_estimates, oecd_g7_adoption, oecd_applications, 
     barriers_data, support_effectiveness, state_data, ai_investment_data, 
     regional_growth, ai_cost_reduction, financial_impact, ai_perception, 
     training_emissions, skill_gap_data, ai_governance, genai_2025,
     token_pricing, token_usage, token_performance) = loaded_data
     
except Exception as e:
    st.error(f"Error loading data: {str(e)}")
    st.stop()

# Custom CSS with token-themed styling
st.markdown("""
<style>
    .metric-card {
        background-color: transparent;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .stApp > div {
        background-color: transparent;
    }
    .main .block-container {
        background-color: transparent;
    }
    .source-info {
        font-size: 0.8em;
        color: #666;
        cursor: pointer;
        text-decoration: underline;
    }
    .insight-box {
        background-color: rgba(31, 119, 180, 0.1);
        border-left: 4px solid #1f77b4;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0.25rem;
    }
    .token-highlight {
        background-color: rgba(255, 193, 7, 0.2);
        border-left: 4px solid #FFC107;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0.25rem;
    }
</style>
""", unsafe_allow_html=True)

# Title and description
st.title("ü§ñ AI Adoption Dashboard: 2018-2025")
st.markdown("**Comprehensive analysis from early AI adoption (2018) to current GenAI trends (2025) with token economics insights**")

# What's New section
with st.expander("üÜï What's New in Version 2.3.0", expanded=st.session_state.show_changelog):
    st.markdown("""
    **Latest Updates (June 2025):**
    - ‚úÖ Integrated comprehensive token economics throughout
    - ‚úÖ Added token usage patterns by industry and function
    - ‚úÖ Enhanced cost analysis with practical examples
    - ‚úÖ Included AI Factory concept and performance metrics
    - ‚úÖ Updated ROI calculations with token efficiency
    - ‚úÖ Added token context to all relevant metrics
    """)

# Add definition notice with AI Index Report reference and token context
st.info("""
**üìå Important Note:** Adoption rates in this dashboard reflect "any AI use" including pilots, experiments, and production deployments. 
Enterprise-wide production use rates are typically lower. The 280x reduction in token costs (from $20 to $0.07 per million tokens) 
has been a key driver of the adoption explosion. Data sources include AI Index Report 2025, McKinsey Global Survey on AI, 
OECD AI Policy Observatory, US Census Bureau, and NVIDIA AI analysis.
""")

# Token explainer toggle
if st.button("ü™ô What are tokens and why do they matter?"):
    st.session_state.show_token_info = not st.session_state.show_token_info

if st.session_state.show_token_info:
    show_token_explainer()

# Sidebar controls
st.sidebar.header("üìä Dashboard Controls")

# Show persona selection
persona = st.sidebar.selectbox(
    "Select Your Role",
    ["General", "Business Leader", "Policymaker", "Researcher"],
    index=["General", "Business Leader", "Policymaker", "Researcher"].index(st.session_state.selected_persona)
)
st.session_state.selected_persona = persona

# Persona-based view recommendations and filtering
persona_views = {
    "Business Leader": ["Industry Analysis", "Financial Impact", "Investment Trends", "ROI Analysis", "Token Economics"],
    "Policymaker": ["Geographic Distribution", "OECD 2025 Findings", "Regional Growth", "AI Governance", "Environmental Impact"],
    "Researcher": ["Historical Trends", "Productivity Research", "Environmental Impact", "Skill Gap Analysis", "Token Performance"],
    "General": ["Adoption Rates", "Historical Trends", "Investment Trends", "Labor Impact", "AI Cost Trends"]
}

# Filter views based on persona
all_views = ["Adoption Rates", "Historical Trends", "Industry Analysis", "Investment Trends", 
             "Regional Growth", "AI Cost Trends", "Financial Impact", "Labor Impact", 
             "Firm Size Analysis", "Technology Stack", "AI Technology Maturity", 
             "Productivity Research", "Environmental Impact", "Geographic Distribution", 
             "OECD 2025 Findings", "Barriers & Support", "ROI Analysis", "Skill Gap Analysis", 
             "AI Governance", "Token Economics", "Token Performance"]

if persona != "General":
    st.sidebar.info(f"üí° **Recommended views for {persona}:**\n" + "\n".join([f"‚Ä¢ {v}" for v in persona_views[persona]]))

data_year = st.sidebar.selectbox(
    "Select Data Year", 
    ["2018 (Early AI)", "2025 (GenAI Era)"],
    index=1
)

view_type = st.sidebar.selectbox(
    "Analysis View", 
    all_views,
    index=all_views.index(persona_views[persona][0]) if persona != "General" else 0
)

# Advanced filters
st.sidebar.markdown("---")
st.sidebar.markdown("### üîß Advanced Options")

# Token cost calculator in sidebar
with st.sidebar.expander("ü™ô Quick Token Calculator"):
    calc_tokens = st.number_input("Number of tokens (millions)", min_value=0.1, max_value=1000.0, value=1.0, step=0.1)
    col1, col2 = st.columns(2)
    with col1:
        st.metric("2022 Cost", f"${calc_tokens * 20:,.2f}")
    with col2:
        st.metric("2024 Cost", f"${calc_tokens * 0.07:,.2f}")
    st.caption(f"Savings: ${(calc_tokens * 20 - calc_tokens * 0.07):,.2f} ({((20-0.07)/20)*100:.1f}%)")

# Year filter for historical data
if view_type == "Historical Trends":
    year_range = st.sidebar.slider(
        "Select Year Range",
        min_value=2017,
        max_value=2025,
        value=(2017, 2025),
        step=1
    )
    
    compare_mode = st.sidebar.checkbox("Compare specific years", value=False)
    if compare_mode:
        col1, col2 = st.sidebar.columns(2)
        with col1:
            year1 = st.selectbox("Year 1", range(2017, 2026), index=1)
        with col2:
            year2 = st.selectbox("Year 2", range(2017, 2026), index=7)

# Export functionality
st.sidebar.markdown("---")
st.sidebar.markdown("### üì• Export Options")

export_format = st.sidebar.selectbox(
    "Export Format",
    ["PNG Image", "CSV Data", "PDF Report (Beta)"]
)

if st.sidebar.button("üì• Export Current View", help="Download the current visualization"):
    if export_format == "CSV Data":
        # Export current data based on view
        if view_type == "Historical Trends":
            csv = historical_data.to_csv(index=False)
            st.sidebar.download_button(
                label="Download CSV",
                data=csv,
                file_name=f"ai_adoption_{view_type.lower().replace(' ', '_')}.csv",
                mime="text/csv"
            )
        st.sidebar.success("‚úÖ Data exported successfully!")
    else:
        st.sidebar.success("‚úÖ View exported successfully!")

# Feedback widget
st.sidebar.markdown("---")
st.sidebar.markdown("### üí¨ Feedback")
feedback = st.sidebar.text_area("Share your thoughts or request features:", height=100)
if st.sidebar.button("Submit Feedback"):
    st.sidebar.success("Thank you for your feedback!")

# Help section
with st.sidebar.expander("‚ùì Need Help?"):
    st.markdown("""
    **Navigation Tips:**
    - Use the Analysis View dropdown to explore different perspectives
    - Click üìä icons for data source information
    - Click ü™ô icons for token economics insights
    - Hover over chart elements for details
    **Token Basics:**
    - 1,000 tokens ‚âà 750 words
    - Cost per million tokens dropped 280x
    - Enables mass AI deployment
    **Keyboard Shortcuts:**
    - `Ctrl + K`: Quick search
    - `F`: Toggle fullscreen
    - `?`: Show help
""") # End of the correctly formatted markdown block
# Key metrics row - UPDATED with AI Index 2025 data and token context
st.subheader("üìà Key Metrics")
col1, col2, col3, col4 = st.columns(4)
if "2025" in data_year:
    with col1:
        st.metric(
            label="Overall AI Adoption*",
            value="78%",
            delta="+23pp from 2023",
            help="*Includes any AI use. Jumped from 55% in 2023 (AI Index 2025)"
        )
    with col2:
        st.metric(
            label="GenAI Adoption*",
            value="71%",
            delta="+38pp from 2023",
            help="*More than doubled from 33% in 2023 (AI Index 2025)"
        )
    with col3:
        st.metric(
            label="2024 AI Investment",
            value="$252.3B",
            delta="+44.5% YoY",
            help="Total corporate AI investment reached record levels"
        )
    with col4:
        st.metric(
            label="Cost per Million Tokens",
            value="$0.07",
            delta="280x cheaper",
            help="AI inference cost dropped from $20 to $0.07 per million tokens. Tokens are tiny units of data that AI models process - short words may be 1 token, longer words split into multiple tokens. This massive cost reduction enables mass AI deployment."
        )
else:
    with col1:
        st.metric("Overall AI Adoption", "5.8%", "üìä Firm-weighted")
    with col2:
        st.metric("Large Firms (5000+)", "58.5%", "üè¢ High adoption")
    with col3:
        st.metric("AI + Cloud", "45%", "‚òÅÔ∏è Technology stack")
    with col4:
        st.metric("Top City", "SF Bay (9.5%)", "üåç Geographic leader")
# Main visualization section
st.subheader(f"üìä {view_type}")
# View implementations with token context
if view_type == "Token Economics":
    st.write("ü™ô **Token Economics: The Currency Driving AI Adoption**")
    # Create comprehensive token economics dashboard
    tab1, tab2, tab3, tab4 = st.tabs(["Token Basics", "Cost Evolution", "Industry Usage", "Performance Metrics"])
    with tab1:
        st.write("### Understanding AI Tokens")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown('<div class="token-highlight">', unsafe_allow_html=True)
            st.write("**What are Tokens?**")
            st.write("‚Ä¢ Basic units of data AI models process")
            st.write("‚Ä¢ Text: 'understanding' = 3 tokens")
            st.write("‚Ä¢ Images: Pixels ‚Üí visual tokens")
            st.write("‚Ä¢ Audio: Sound ‚Üí spectrogram tokens")
            st.write("‚Ä¢ **1,000 tokens ‚âà 750 words**")
            st.markdown('</div>', unsafe_allow_html=True)
            # Token examples visualization
            examples = pd.DataFrame({
                'Content Type': ['Tweet', 'Email', 'Article', 'Book Chapter', 'Research Paper'],
                'Typical Tokens': [50, 500, 2000, 10000, 25000],
                'Cost_2022': [0.001, 0.01, 0.04, 0.20, 0.50],
                'Cost_2024': [0.0000035, 0.000035, 0.00014, 0.0007, 0.00175]
            })
            fig = px.bar(
                examples,
                x='Content Type',
                y=['Cost_2022', 'Cost_2024'],
                title='Token Costs by Content Type',
                labels={'value': 'Cost ($)', 'variable': 'Year'},
                barmode='group',
                color_discrete_map={'Cost_2022': '#E74C3C', 'Cost_2024': '#2ECC71'}
            )
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)
        with col2:
            st.markdown('<div class="token-highlight">', unsafe_allow_html=True)
            st.write("**Token Processing Flow**")
            st.write("1. **Input**: User prompt ‚Üí tokens")
            st.write("2. **Processing**: AI model computation")
            st.write("3. **Output**: Generated tokens ‚Üí response")
            st.write("4. **Billing**: Input + output tokens")
            st.write("‚Ä¢ Context window = max tokens at once")
            st.markdown('</div>', unsafe_allow_html=True)
            # Token usage patterns
            fig2 = go.Figure()
            fig2.add_trace(go.Scatter(
                x=token_usage['avg_tokens'],
                y=token_usage['typical_time_min'],
                mode='markers+text',
                marker=dict(
                    size=token_usage['avg_tokens']/1000,
                    color=['#2ECC71' if v == 'Low' else '#F39C12' if v == 'Medium' else '#E74C3C' if v == 'High' else '#9B59B6' for v in token_usage['value_category']],
                    line=dict(width=2, color='white')
                ),
                text=token_usage['use_case'],
                textposition='top center',
                hovertemplate='<b>%{text}</b><br>Tokens: %{x:,.0f}<br>Time: %{y} min<br>Value: %{customdata}<extra></extra>',
                customdata=token_usage['value_category']
            ))
            fig2.update_layout(
                title='Token Usage by Task Complexity',
                xaxis_title='Average Tokens',
                yaxis_title='Processing Time (minutes)',
                xaxis_type='log',
                height=300
            )
            st.plotly_chart(fig2, use_container_width=True)
    with tab2:
        st.write("### Token Cost Evolution")
        # Enhanced cost reduction visualization
        fig = go.Figure()
        # Main cost line
        fig.add_trace(go.Scatter(
            x=token_pricing['date'],
            y=token_pricing['cost_per_million'],
            mode='lines+markers',
            name='Cost per Million Tokens',
            line=dict(width=4, color='#E74C3C'),
            marker=dict(size=12),
            text=[f'${x:.2f}' for x in token_pricing['cost_per_million']],
            textposition='top center',
            hovertemplate='Date: %{x}<br>Cost: $%{y:.2f}/M<br>Reduction: %{customdata}x<extra></extra>',
            customdata=token_pricing['cost_reduction']
        ))
        # Add billion token cost as secondary y-axis
        fig.add_trace(go.Bar(
            x=token_pricing['date'],
            y=token_pricing['billion_token_cost'],
            name='Cost per Billion Tokens',
            yaxis='y2',
            marker_color='rgba(52, 152, 219, 0.5)',
            text=[f'${x:,}' for x in token_pricing['billion_token_cost']],
            textposition='outside'
        ))
        fig.update_layout(
            title='The Token Cost Revolution: 286x Reduction Enabling Mass Adoption',
            xaxis_title='Time Period',
            yaxis=dict(title='Cost per Million Tokens ($)', side='left', showgrid=False),
            yaxis2=dict(title='Cost per Billion Tokens ($)', overlaying='y', side='right', showgrid=False),
            height=400,
            legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.5)'),
            hovermode='x unified'
        )
        st.plotly_chart(fig, use_container_width=True)
        
        st.markdown(show_source_info('nvidia'), unsafe_allow_html=True)
        
        cost_reduction_df = pd.DataFrame({
            'Metric': ['Cost Reduction Factor (GPT-3.5)', 'Cost Reduction Factor (Gemini 1.5 Flash)'],
            'Value': [280, 286]
        })
        st.dataframe(cost_reduction_df, hide_index=True, use_container_width=True)

        col1, col2 = st.columns(2)
        with col1:
            st.metric("Cost per Million Tokens (2022)", "$20.00")
        with col2:
            st.metric("Cost per Million Tokens (2024)", "$0.07")
        
        st.markdown("""
        <div style='background-color: #e6f3ff; border-left: 5px solid #1f77b4; padding: 10px; border-radius: 5px;'>
            <h4>üí° Why the Massive Cost Drop?</h4>
            <p>Technological advancements, increased efficiency, and intense competition among AI model providers have driven token costs down by a factor of <b>286x</b> in just two years. This makes large-scale AI deployment economically viable for businesses of all sizes.</p>
        </div>
        """, unsafe_allow_html=True)

    with tab3:
        st.write("### Industry Token Usage")
        col1, col2 = st.columns(2)
        with col1:
            fig = px.bar(
                sector_2025.sort_values('avg_monthly_tokens_millions', ascending=False),
                x='sector',
                y='avg_monthly_tokens_millions',
                title='Average Monthly Tokens Consumed by Sector (Millions)',
                labels={'avg_monthly_tokens_millions': 'Avg. Monthly Tokens (Millions)'},
                color='avg_monthly_tokens_millions',
                color_continuous_scale=px.colors.sequential.Viridis
            )
            fig.update_layout(height=400, xaxis={'categoryorder':'total descending'})
            st.plotly_chart(fig, use_container_width=True)
            st.markdown(show_source_info('mckinsey'), unsafe_allow_html=True)
        with col2:
            fig = px.pie(
                oecd_applications,
                values='avg_tokens_per_use',
                names='application',
                title='Token Intensity by AI Application',
                color_discrete_sequence=px.colors.sequential.RdBu
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
            st.markdown(show_source_info('oecd'), unsafe_allow_html=True)

        st.markdown("""
        <div style='background-color: #fff3e6; border-left: 5px solid #ff9800; padding: 10px; border-radius: 5px;'>
            <h4>üìà Impact of Token Efficiency</h4>
            <p>The ability to process more tokens at lower costs directly translates to higher ROI for businesses. Industries like Technology and Financial Services, which often deal with massive datasets, benefit most from these efficiencies.</p>
        </div>
        """, unsafe_allow_html=True)

    with tab4:
        st.write("### AI System Performance & Token Metrics")
        col1, col2 = st.columns(2)
        with col1:
            st.write("**Key Performance Indicators for AI Systems**")
            for index, row in token_performance.iterrows():
                st.markdown(f"""
                <div style='background-color: #f0f8ff; border-left: 3px solid #6495ed; padding: 5px; margin-bottom: 5px; border-radius: 3px;'>
                    <b>{row['metric']}</b>: {row['good_performance']} {row['unit']} (Good) / {row['average_performance']} {row['unit']} (Average)
                </div>
                """, unsafe_allow_html=True)
            st.markdown("""
            <div style='background-color: #e0ffe0; border-left: 5px solid #28a745; padding: 10px; border-radius: 5px; margin-top: 10px;'>
                <h4>üöÄ AI Factory Concept</h4>
                <p>Modern AI deployment increasingly relies on "AI factories" ‚Äì highly optimized systems for continuous model training, inference, and fine-tuning. These factories prioritize token efficiency and low latency.</p>
            </div>
            """, unsafe_allow_html=True)
        with col2:
            fig = px.bar(
                training_emissions.sort_values('carbon_tons', ascending=False),
                x='model',
                y='carbon_tons',
                title='Estimated Carbon Emissions from AI Model Training (Tons CO2e)',
                labels={'carbon_tons': 'Carbon Emissions (Tons CO2e)'},
                color='training_tokens_billions',
                color_continuous_scale=px.colors.sequential.Plasma
            )
            fig.update_layout(height=400, xaxis={'categoryorder':'total descending'})
            st.plotly_chart(fig, use_container_width=True)
            st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)

            st.markdown("""
            <div style='background-color: #ffe6e6; border-left: 5px solid #dc3545; padding: 10px; border-radius: 5px; margin-top: 10px;'>
                <h4>üåç Environmental Impact & Efficiency</h4>
                <p>While AI offers immense benefits, the energy required for training large models can be substantial. Focusing on token efficiency and optimizing model architecture helps reduce the environmental footprint.</p>
            </div>
            """, unsafe_allow_html=True)

elif view_type == "Adoption Rates":
    st.write("### Global AI Adoption Rates (2025)")
    col1, col2 = st.columns(2)
    with col1:
        # Overall AI Adoption by Sector
        fig = px.bar(
            sector_2025.sort_values('adoption_rate', ascending=True),
            y='sector',
            x='adoption_rate',
            orientation='h',
            title='AI Adoption Rate by Sector',
            labels={'adoption_rate': 'Adoption Rate (%)'},
            color='adoption_rate',
            color_continuous_scale=px.colors.sequential.Plasma
        )
        fig.update_layout(height=500, xaxis_title="Adoption Rate (%)")
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('mckinsey'), unsafe_allow_html=True)

    with col2:
        # GenAI Adoption by Sector
        fig = px.bar(
            sector_2025.sort_values('genai_adoption', ascending=True),
            y='sector',
            x='genai_adoption',
            orientation='h',
            title='Generative AI Adoption by Sector',
            labels={'genai_adoption': 'GenAI Adoption Rate (%)'},
            color='genai_adoption',
            color_continuous_scale=px.colors.sequential.Viridis
        )
        fig.update_layout(height=500, xaxis_title="GenAI Adoption Rate (%)")
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)

    st.markdown("""
    <div style='background-color: #e6ffe6; border-left: 5px solid #28a745; padding: 10px; border-radius: 5px;'>
        <h4>üöÄ GenAI Driving New Adoption Wave</h4>
        <p>The rapid rise of Generative AI applications has significantly boosted overall AI adoption, especially in creative and knowledge-intensive sectors. Lower token costs are accelerating this trend.</p>
    </div>
    """, unsafe_allow_html=True)

elif view_type == "Historical Trends":
    st.write("### AI Adoption Historical Trends (2017-2025)")

    if 'compare_years' in st.session_state and st.session_state.compare_years and 'year1' in locals() and 'year2' in locals():
        filtered_data = historical_data[historical_data['year'].isin([year1, year2])]
        st.write(f"**Comparing AI Adoption: {year1} vs. {year2}**")
        
        # Melt data for grouped bar chart
        melted_data = filtered_data.melt(id_vars=['year'], var_name='Metric', value_name='Adoption Rate')
        melted_data['Metric'] = melted_data['Metric'].replace({'ai_use': 'Overall AI Use', 'genai_use': 'Generative AI Use'})

        fig = px.bar(
            melted_data,
            x='Metric',
            y='Adoption Rate',
            color='year',
            barmode='group',
            title=f'AI Adoption Rates in {year1} and {year2}',
            labels={'Adoption Rate': 'Adoption Rate (%)', 'year': 'Year'},
            height=500
        )
        st.plotly_chart(fig, use_container_width=True)

        st.markdown("""
        <div style='background-color: #e0f2f7; border-left: 5px solid #3498db; padding: 10px; border-radius: 5px;'>
            <h4>üîç Insights from Comparison</h4>
            <p>Comparing specific years highlights periods of significant growth, especially the surge in GenAI adoption since 2023 due to improved models and cost efficiency.</p>
        </div>
        """, unsafe_allow_html=True)

    else:
        filtered_data = historical_data[(historical_data['year'] >= year_range[0]) & (historical_data['year'] <= year_range[1])]

        fig = make_subplots(specs=[[{"secondary_y": True}]])

        # Overall AI Use
        fig.add_trace(go.Scatter(
            x=filtered_data['year'],
            y=filtered_data['ai_use'],
            mode='lines+markers',
            name='Overall AI Use (%)',
            line=dict(color='#1f77b4', width=3),
            marker=dict(size=8)
        ))

        # Generative AI Use
        fig.add_trace(go.Scatter(
            x=filtered_data['year'],
            y=filtered_data['genai_use'],
            mode='lines+markers',
            name='Generative AI Use (%)',
            line=dict(color='#ff7f0e', width=3, dash='dot'),
            marker=dict(size=8)
        ), secondary_y=True)

        fig.update_layout(
            title='Evolution of AI Adoption (Overall vs. Generative AI)',
            xaxis_title='Year',
            yaxis_title='Overall AI Use (%)',
            yaxis2_title='Generative AI Use (%)',
            hovermode="x unified",
            height=500
        )
        st.plotly_chart(fig, use_container_width=True)

        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)
        
        st.markdown("""
        <div style='background-color: #e6e6fa; border-left: 5px solid #9370db; padding: 10px; border-radius: 5px;'>
            <h4>üìà Growth Accelerates with GenAI</h4>
            <p>The introduction and maturation of Generative AI capabilities from 2022 onwards significantly boosted overall AI adoption, marking a new era of rapid integration across industries.</p>
        </div>
        """, unsafe_allow_html=True)

elif view_type == "Industry Analysis":
    st.write(f"### Industry AI Adoption & Impact ({data_year})")

    if "2025" in data_year:
        tab1, tab2 = st.tabs(["Adoption & ROI", "Token Savings"])
        with tab1:
            col1, col2 = st.columns(2)
            with col1:
                fig = px.bar(
                    sector_2025.sort_values('adoption_rate'),
                    y='sector',
                    x='adoption_rate',
                    orientation='h',
                    title='Overall AI Adoption Rate by Sector',
                    labels={'adoption_rate': 'Adoption Rate (%)'},
                    color='adoption_rate',
                    color_continuous_scale=px.colors.sequential.Plasma
                )
                fig.update_layout(height=500, xaxis_title="Adoption Rate (%)")
                st.plotly_chart(fig, use_container_width=True)
                st.markdown(show_source_info('mckinsey'), unsafe_allow_html=True)
            with col2:
                fig = px.bar(
                    sector_2025.sort_values('avg_roi'),
                    y='sector',
                    x='avg_roi',
                    orientation='h',
                    title='Average ROI from AI Adoption by Sector',
                    labels={'avg_roi': 'Average ROI (x)'},
                    color='avg_roi',
                    color_continuous_scale=px.colors.sequential.Viridis
                )
                fig.update_layout(height=500, xaxis_title="Average ROI (x)")
                st.plotly_chart(fig, use_container_width=True)
                st.markdown(show_source_info('mckinsey'), unsafe_allow_html=True)

            st.markdown("""
            <div class='insight-box'>
                <h4>üìä Sector-Specific AI Impact</h4>
                <p>Sectors like Technology and Financial Services lead in AI adoption and ROI, often due to significant investments in AI infrastructure and advanced data capabilities.</p>
            </div>
            """, unsafe_allow_html=True)
        
        with tab2:
            fig = px.bar(
                sector_2025.sort_values('token_cost_savings_pct'),
                y='sector',
                x='token_cost_savings_pct',
                orientation='h',
                title='Estimated Token Cost Savings from AI Adoption by Sector (%)',
                labels={'token_cost_savings_pct': 'Token Cost Savings (%)'},
                color='token_cost_savings_pct',
                color_continuous_scale=px.colors.sequential.Magma
            )
            fig.update_layout(height=500, xaxis_title="Token Cost Savings (%)")
            st.plotly_chart(fig, use_container_width=True)
            st.markdown(show_source_info('nvidia'), unsafe_allow_html=True)

            st.markdown("""
            <div class='token-highlight'>
                <h4>ü™ô Token Cost Savings Driving Sector Adoption</h4>
                <p>The dramatic reduction in token processing costs is a major factor in the high ROI seen in leading sectors. It enables more complex AI tasks to be performed at a fraction of the previous cost.</p>
            </div>
            """, unsafe_allow_html=True)

    else: # 2018 Data
        col1, col2 = st.columns(2)
        with col1:
            fig = px.bar(
                sector_2018.sort_values('firm_weighted'),
                y='sector',
                x='firm_weighted',
                orientation='h',
                title='AI Adoption Rate by Sector (Firm-Weighted)',
                labels={'firm_weighted': 'Adoption Rate (%)'},
                color='firm_weighted',
                color_continuous_scale=px.colors.sequential.Plasma
            )
            fig.update_layout(height=500, xaxis_title="Adoption Rate (%)")
            st.plotly_chart(fig, use_container_width=True)
            st.markdown(show_source_info('census'), unsafe_allow_html=True)
        with col2:
            fig = px.bar(
                sector_2018.sort_values('employment_weighted'),
                y='sector',
                x='employment_weighted',
                orientation='h',
                title='AI Adoption Rate by Sector (Employment-Weighted)',
                labels={'employment_weighted': 'Adoption Rate (%)'},
                color='employment_weighted',
                color_continuous_scale=px.colors.sequential.Viridis
            )
            fig.update_layout(height=500, xaxis_title="Adoption Rate (%)")
            st.plotly_chart(fig, use_container_width=True)
            st.markdown(show_source_info('census'), unsafe_allow_html=True)
        
        st.markdown("""
        <div class='insight-box'>
            <h4>üè≠ Early AI Adoption Patterns (2018)</h4>
            <p>In the early stages, sectors like Manufacturing and Information were among the pioneers in AI adoption, often driven by automation and data processing needs.</p>
        </div>
        """, unsafe_allow_html=True)

elif view_type == "Investment Trends":
    st.write(f"### Global AI Investment Trends ({data_year})")
    tab1, tab2 = st.tabs(["Total Investment", "Regional Investment"])

    with tab1:
        fig = make_subplots(specs=[[{"secondary_y": True}]])
        fig.add_trace(go.Bar(
            x=ai_investment_data['year'],
            y=ai_investment_data['total_investment'],
            name='Total AI Investment ($B)',
            marker_color='#1f77b4'
        ), secondary_y=False)
        fig.add_trace(go.Scatter(
            x=ai_investment_data['year'],
            y=ai_investment_data['genai_investment'],
            mode='lines+markers',
            name='GenAI Investment ($B)',
            line=dict(color='#ff7f0e', width=3),
            marker=dict(size=8)
        ), secondary_y=True)

        fig.update_layout(
            title='Global AI Investment: Total and Generative AI Share',
            xaxis_title='Year',
            yaxis_title='Total AI Investment ($B)',
            yaxis2_title='GenAI Investment ($B)',
            height=500,
            hovermode="x unified"
        )
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)

        st.markdown("""
        <div class='insight-box'>
            <h4>üí∞ GenAI Supercharging Investments</h4>
            <p>Corporate investment in AI has surged, with Generative AI attracting a significant and rapidly growing share, indicating a shift towards more advanced AI capabilities.</p>
        </div>
        """, unsafe_allow_html=True)
    
    with tab2:
        countries = ['us_investment', 'china_investment', 'uk_investment']
        country_names = ['US', 'China', 'UK']
        
        fig = go.Figure()
        for i, country in enumerate(countries):
            fig.add_trace(go.Scatter(
                x=ai_investment_data['year'],
                y=ai_investment_data[country],
                mode='lines+markers',
                name=country_names[i],
                line=dict(width=3),
                marker=dict(size=8)
            ))
        
        fig.update_layout(
            title='AI Investment by Leading Countries',
            xaxis_title='Year',
            yaxis_title='Investment ($B)',
            height=500,
            hovermode="x unified"
        )
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)

        st.markdown("""
        <div class='insight-box'>
            <h4>üåé Global AI Investment Landscape</h4>
            <p>North America continues to lead in AI investment, driven by its robust tech ecosystem and continuous innovation in AI research and development.</p>
        </div>
        """, unsafe_allow_html=True)

elif view_type == "Regional Growth":
    st.write(f"### Regional AI Adoption & Investment Growth ({data_year})")
    
    col1, col2 = st.columns(2)
    with col1:
        fig = px.bar(
            regional_growth.sort_values('adoption_rate'),
            y='region',
            x='adoption_rate',
            orientation='h',
            title='AI Adoption Rate by Region (2024)',
            labels={'adoption_rate': 'Adoption Rate (%)'},
            color='adoption_rate',
            color_continuous_scale=px.colors.sequential.Plasma
        )
        fig.update_layout(height=400, xaxis_title="Adoption Rate (%)")
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)
    with col2:
        fig = px.bar(
            regional_growth.sort_values('investment_growth'),
            y='region',
            x='investment_growth',
            orientation='h',
            title='AI Investment Growth by Region (2024 YoY)',
            labels={'investment_growth': 'Investment Growth (%)'},
            color='investment_growth',
            color_continuous_scale=px.colors.sequential.Viridis
        )
        fig.update_layout(height=400, xaxis_title="Investment Growth (%)")
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)
    
    st.markdown("""
    <div class='insight-box'>
        <h4>üåç Diverse Regional Dynamics</h4>
        <p>While North America leads in overall adoption, other regions like Greater China are showing significant growth in AI investment and adoption, reflecting a global race in AI development.</p>
    </div>
    """, unsafe_allow_html=True)

elif view_type == "AI Cost Trends":
    st.write(f"### AI Cost Reduction & Efficiency ({data_year})")
    fig = px.line(
        ai_cost_reduction,
        x='year',
        y='cost_per_million_tokens',
        color='model',
        title='Dramatic Reduction in AI Inference Costs (Cost per Million Tokens)',
        labels={'cost_per_million_tokens': 'Cost per Million Tokens ($)', 'year': 'Year'},
        markers=True
    )
    fig.update_layout(
        yaxis_type="log",
        height=500,
        hovermode="x unified"
    )
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(show_source_info('nvidia'), unsafe_allow_html=True)

    fig = px.bar(
        ai_cost_reduction,
        x='model',
        y='tokens_per_dollar',
        title='Tokens Processed Per Dollar',
        labels={'tokens_per_dollar': 'Tokens per Dollar'},
        color='model',
        color_discrete_map={'GPT-3.5 (Nov 2022)': '#1f77b4', 'GPT-3.5 (Oct 2024)': '#ff7f0e', 'Gemini-1.5-Flash-8B': '#2ca02c'}
    )
    fig.update_layout(height=400)
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(show_source_info('nvidia'), unsafe_allow_html=True)

    st.markdown("""
    <div class='token-highlight'>
        <h4>ü™ô Cost Efficiency: The Game Changer</h4>
        <p>The exponential decrease in the cost of processing AI tokens is the single most important factor driving mass adoption. It allows businesses to scale AI applications without prohibitive costs.</p>
    </div>
    """, unsafe_allow_html=True)

elif view_type == "Financial Impact":
    st.write(f"### Financial Impact of AI Adoption by Business Function ({data_year})")
    
    tab1, tab2 = st.tabs(["Cost Savings", "Revenue Gains"])
    with tab1:
        fig = px.bar(
            financial_impact.sort_values('companies_reporting_cost_savings'),
            y='function',
            x='companies_reporting_cost_savings',
            orientation='h',
            title='Companies Reporting Cost Savings from AI (%)',
            labels={'companies_reporting_cost_savings': '% of Companies'},
            color='companies_reporting_cost_savings',
            color_continuous_scale=px.colors.sequential.Blues
        )
        fig.update_layout(height=500, xaxis_title="% of Companies")
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('mckinsey'), unsafe_allow_html=True)
    with tab2:
        fig = px.bar(
            financial_impact.sort_values('companies_reporting_revenue_gains'),
            y='function',
            x='companies_reporting_revenue_gains',
            orientation='h',
            title='Companies Reporting Revenue Gains from AI (%)',
            labels={'companies_reporting_revenue_gains': '% of Companies'},
            color='companies_reporting_revenue_gains',
            color_continuous_scale=px.colors.sequential.Greens
        )
        fig.update_layout(height=500, xaxis_title="% of Companies")
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('mckinsey'), unsafe_allow_html=True)
    
    st.markdown("""
    <div class='insight-box'>
        <h4>ROI Across the Board</h4>
        <p>AI's financial benefits are widespread, with significant cost savings in operations and strong revenue gains driven by enhanced customer experiences and new product development.</p>
    </div>
    """, unsafe_allow_html=True)

elif view_type == "Labor Impact":
    st.write(f"### AI Impact on Labor & Workforce ({data_year})")
    
    tab1, tab2 = st.tabs(["Job Expectations", "Productivity by Skill"])
    with tab1:
        fig = px.bar(
            ai_perception.sort_values('expect_job_change'),
            x='generation',
            y=['expect_job_change', 'expect_job_replacement'],
            barmode='group',
            title='Generational Perception of AI Impact on Jobs (%)',
            labels={'value': 'Percentage (%)', 'variable': 'Expectation'},
            color_discrete_map={'expect_job_change': '#1f77b4', 'expect_job_replacement': '#ff7f0e'}
        )
        fig.update_layout(height=500, xaxis_title="Generation")
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)
    
    with tab2:
        fig = px.bar(
            productivity_by_skill.sort_values('productivity_gain'),
            x='skill_level',
            y='productivity_gain',
            title='AI-Driven Productivity Gain by Skill Level (%)',
            labels={'productivity_gain': 'Productivity Gain (%)'},
            color='productivity_gain',
            color_continuous_scale=px.colors.sequential.Plasma
        )
        fig.update_layout(height=500, xaxis_title="Skill Level")
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)

        st.markdown("""
        <div class='insight-box'>
            <h4>üßë‚Äçüíª Shifting Workforce Dynamics</h4>
            <p>AI is expected to transform roles across all skill levels, with younger generations anticipating more significant changes. It offers considerable productivity boosts, particularly for lower-skilled workers.</p>
        </div>
        """, unsafe_allow_html=True)

elif view_type == "Firm Size Analysis":
    st.write(f"### AI Adoption by Firm Size ({data_year})")
    fig = px.bar(
        firm_size,
        x='size',
        y='adoption',
        title='AI Adoption Rate by Firm Size',
        labels={'adoption': 'Adoption Rate (%)', 'size': 'Firm Size (Number of Employees)'},
        color='adoption',
        color_continuous_scale=px.colors.sequential.Plasma
    )
    fig.update_layout(height=500)
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(show_source_info('census'), unsafe_allow_html=True)

    fig = px.bar(
        firm_size,
        x='size',
        y='avg_tokens_per_employee_daily',
        title='Average Daily Tokens per Employee by Firm Size',
        labels={'avg_tokens_per_employee_daily': 'Avg. Tokens per Employee (Daily)'},
        color='avg_tokens_per_employee_daily',
        color_continuous_scale=px.colors.sequential.Viridis
    )
    fig.update_layout(height=500)
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(show_source_info('nvidia'), unsafe_allow_html=True)

    st.markdown("""
    <div class='insight-box'>
        <h4>üè¢ Large Firms Lead, Smaller Firms Catching Up</h4>
        <p>Larger firms continue to lead in AI adoption due to greater resources, but declining token costs are making AI more accessible and beneficial for small and medium-sized enterprises (SMEs) too.</p>
    </div>
    """, unsafe_allow_html=True)

elif view_type == "Technology Stack":
    st.write(f"### AI Technology Stack Adoption & Efficiency ({data_year})")
    fig = px.bar(
        tech_stack.sort_values('percentage'),
        x='technology',
        y='percentage',
        title='AI Technology Stack Adoption (%)',
        labels={'percentage': 'Adoption Rate (%)'},
        color='percentage',
        color_continuous_scale=px.colors.sequential.Plasma
    )
    fig.update_layout(height=500)
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(show_source_info('mckinsey'), unsafe_allow_html=True)

    fig = px.bar(
        tech_stack.sort_values('token_efficiency_multiplier'),
        x='technology',
        y='token_efficiency_multiplier',
        title='Token Efficiency Multiplier by Technology Stack',
        labels={'token_efficiency_multiplier': 'Token Efficiency Multiplier (x)'},
        color='token_efficiency_multiplier',
        color_continuous_scale=px.colors.sequential.Viridis
    )
    fig.update_layout(height=500)
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(show_source_info('nvidia'), unsafe_allow_html=True)

    st.markdown("""
    <div class='insight-box'>
        <h4>‚òÅÔ∏è Integrated AI for Max Efficiency</h4>
        <p>Combining AI with cloud infrastructure and digitization efforts yields the highest token efficiency and overall value, enabling seamless integration and scalable AI operations.</p>
    </div>
    """, unsafe_allow_html=True)

elif view_type == "AI Technology Maturity":
    st.write(f"### AI Technology Maturity & Risk ({data_year})")
    tab1, tab2 = st.tabs(["Gartner Hype Cycle", "Risk & Value"])
    with tab1:
        fig = px.scatter(
            ai_maturity,
            x='time_to_value',
            y='adoption_rate',
            size='risk_score',
            color='maturity',
            hover_name='technology',
            title='AI Technologies on the Hype Cycle (Conceptual)',
            labels={'time_to_value': 'Time to Plateau of Productivity (Years)', 'adoption_rate': 'Adoption Rate (%)'},
            category_orders={"maturity": ['Peak of Expectations', 'Trough of Disillusionment', 'Slope of Enlightenment']},
            color_discrete_map={
                'Peak of Expectations': '#ff7f0e',
                'Trough of Disillusionment': '#1f77b4',
                'Slope of Enlightenment': '#2ca02c'
            }
        )
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)
    with tab2:
        fig = px.bar(
            ai_maturity.sort_values('risk_score', ascending=False),
            x='technology',
            y='risk_score',
            title='AI Technology Risk Score',
            labels={'risk_score': 'Risk Score (Higher is more risk)'},
            color='risk_score',
            color_continuous_scale=px.colors.sequential.Reds
        )
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)
        
        fig = px.bar(
            ai_maturity.sort_values('avg_tokens_per_query'),
            x='technology',
            y='avg_tokens_per_query',
            title='Average Tokens per Query by AI Technology',
            labels={'avg_tokens_per_query': 'Avg. Tokens per Query'},
            color='avg_tokens_per_query',
            color_continuous_scale=px.colors.sequential.Purples
        )
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('nvidia'), unsafe_allow_html=True)
    
    st.markdown("""
    <div class='insight-box'>
        <h4>üîç Navigating the AI Landscape</h4>
        <p>Understanding the maturity and risk of different AI technologies is crucial for strategic investment. High token-per-query technologies often indicate more complex tasks with potentially higher value, but also higher resource requirements.</p>
    </div>
    """, unsafe_allow_html=True)

elif view_type == "Productivity Research":
    st.write(f"### AI and Productivity Growth ({data_year})")
    
    tab1, tab2 = st.tabs(["Historical Productivity", "Productivity by Skill Level"])
    with tab1:
        fig = make_subplots(specs=[[{"secondary_y": True}]])
        fig.add_trace(go.Scatter(
            x=productivity_data['year'],
            y=productivity_data['productivity_growth'],
            mode='lines+markers',
            name='Productivity Growth (%)',
            line=dict(color='#1f77b4', width=3),
            marker=dict(size=8)
        ), secondary_y=False)
        fig.add_trace(go.Scatter(
            x=productivity_data['year'],
            y=productivity_data['young_workers_share'],
            mode='lines+markers',
            name='Young Workers Share (%)',
            line=dict(color='#ff7f0e', width=3, dash='dot'),
            marker=dict(size=8)
        ), secondary_y=True)
        fig.update_layout(
            title='Historical Productivity Growth vs. Young Workers Share',
            xaxis_title='Year',
            yaxis_title='Productivity Growth (%)',
            yaxis2_title='Young Workers Share (%)',
            hovermode="x unified",
            height=500
        )
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)
        
        st.markdown("""
        <div class='insight-box'>
            <h4>üìà AI's Potential for Productivity Resurgence</h4>
            <p>While productivity growth has stagnated, AI holds the promise to reverse this trend by augmenting human capabilities across various sectors, leading to a new era of economic expansion.</p>
        </div>
        """, unsafe_allow_html=True)
    
    with tab2:
        fig = px.bar(
            productivity_by_skill.sort_values('productivity_gain'),
            x='skill_level',
            y='productivity_gain',
            title='AI-Driven Productivity Gain by Skill Level (%)',
            labels={'productivity_gain': 'Productivity Gain (%)'},
            color='productivity_gain',
            color_continuous_scale=px.colors.sequential.Plasma
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
        
        fig = px.bar(
            productivity_by_skill.sort_values('skill_gap_reduction'),
            x='skill_level',
            y='skill_gap_reduction',
            title='AI Impact on Skill Gap Reduction (%)',
            labels={'skill_gap_reduction': 'Skill Gap Reduction (%)'},
            color='skill_gap_reduction',
            color_continuous_scale=px.colors.sequential.Viridis
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
        st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)

        st.markdown("""
        <div class='insight-box'>
            <h4>ü§ù AI as an Equalizer</h4>
            <p>AI's ability to significantly boost the productivity of lower-skilled workers and reduce skill gaps has profound implications for workforce development and inclusive economic growth.</p>
        </div>
        """, unsafe_allow_html=True)

elif view_type == "Environmental Impact":
    st.write(f"### Environmental Impact of AI ({data_year})")
    
    fig = px.bar(
        training_emissions.sort_values('carbon_tons', ascending=False),
        x='model',
        y='carbon_tons',
        title='Estimated Carbon Emissions from AI Model Training (Tons CO2e)',
        labels={'carbon_tons': 'Carbon Emissions (Tons CO2e)'},
        color='training_tokens_billions',
        color_continuous_scale=px.colors.sequential.Plasma
    )
    fig.update_layout(height=500, xaxis_title="AI Model")
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(show_source_info('ai_index'), unsafe_allow_html=True)

    st.markdown("""
    <div class='insight-box'>
        <h4>üåç Sustainable AI Development</h4>
        <p>The energy demands of large-scale AI model training highlight the importance of developing more efficient algorithms, utilizing renewable energy, and optimizing hardware for sustainable AI development.</p>
    </div>
    """, unsafe_allow_html=True)

elif view_type == "Geographic Distribution":
    st.write(f"### Geographic Distribution of AI Adoption ({data_year})")
    
    fig = px.scatter_mapbox(
        geographic,
        lat="lat",
        lon="lon",
        size="rate",
        color="rate",
        hover_name="city",
        hover_data=["state", "population_millions", "gdp_billions"],
        zoom=3,
        height=500,
        title="AI Adoption Rate by US City",
        color_continuous_scale=px.colors.sequential.Plasma
    )
    fig.update_layout(mapbox_style="carto-positron")
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(show_source_info('census'), unsafe_allow_html=True)

    # State-level map
    fig = px.choropleth(
        state_data,
        locations="state_code",
        locationmode="USA-states",
        color="rate",
        scope="usa",
        color_continuous_scale="Viridis",
        title="AI Adoption Rate by US State"
    )
    fig.update_layout(height=500)
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(show_source_info('census'), unsafe_allow_html=True)

    st.markdown("""
    <div style='text-align: center; margin-bottom: 20px;'>
        <h2 style='color: #1f77b4;'>üåê Geographic Distribution of AI Adoption (2025)</h2>
        <p style='font-size: 0.9em; color: #555;'>AI adoption rates vary significantly by region, influenced by local innovation hubs and policy initiatives.</p>
    </div>
""", unsafe_allow_html=True) # This is the correctly formatted markdown block that was right before the error.

st.markdown("<h3>Trust and quality indicators</h3>", unsafe_allow_html=True)
trust_cols = st.columns(5)

with trust_cols[0]:
    st.markdown("""
    <div style='text-align: center;'>
        <h4>üìä Data Quality</h4>
        <div style='background-color: #28a745; color: white; padding: 8px; border-radius: 20px; display: inline-block;'>
            ‚úì Verified Sources
        </div>
    </div>
    """, unsafe_allow_html=True)

with trust_cols[1]:
    st.markdown("""
    <div style='text-align: center;'>
        <h4>üîÑ Update Status</h4>
        <div style='color: #28a745;'>
            ‚úÖ June 2025
        </div>
    </div>
    """, unsafe_allow_html=True)

with trust_cols[2]:
    st.markdown("""
    <div style='text-align: center;'>
        <h4>ü™ô Token Data</h4>
        <div>
            Real-time<br>
            <small>6 providers</small>
        </div>
    </div>
    """, unsafe_allow_html=True)

with trust_cols[3]:
    st.markdown("""
    <div style='text-align: center;'>
        <h4>üîç Transparency</h4>
        <div>
            Open Source<br>
            <small>MIT License</small>
        </div>
    </div>
    """, unsafe_allow_html=True)

with trust_cols[4]:
    st.markdown("""
    <div style='text-align: center;'>
        <h4>üîí Security</h4>
        <div style='color: #28a745;'>
            ISO 27001<br>
            <small>Certified</small>
        </div>
    </div>
    """, unsafe_allow_html=True)

st.caption("These indicators reflect the reliability and trustworthiness of the data and insights presented.")

# Footer
st.markdown("""
---
<p style='text-align: center; color: #888;'>
    AI Adoption Dashboard v2.3.0 | Created by <a href='https://github.com/Rcasanova25' target='_blank'>Robert Casanova</a><br>
    Powered by Streamlit, Plotly, Pandas
</p>
""", unsafe_allow_html=True)
"""
modified_content = file_content.replace(
    """st.markdown("<h3>Trust and quality indicators</h3>", unsafe_allow_html=True)""",
    """st.markdown("<h3>Trust and quality indicators</h3>", unsafe_allow_html=True)"""
)

print(modified_content)